# LLaMA

- [Introducing LLaMA: A foundational, 65-billion-parameter large language model](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)

## Reproductions

1. GPTQ-for-LLaMa
2. llama.cpp
3. LLaMA-Adapter V2
4. Lit-LLaMa
5. StackLLaMa
6. The Bloke, `alpaca-lora-65B-GGML`

## Links

- [Read The Paper](https://arxiv.org/abs/2302.13971)
- [Read The Model Card](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md)
- [Apply For Access To LLaMa](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform)

## Relevant Reading

- [A LLaMa That Spits Out Posts: OUR TEST OF META'S AI](https://les-enovateurs.com/llama-meta-ai-test)
- [A Stranger Shares A Torrent Link For The Models, Transfomer, "Save bandwidth by using a torrent to distribute more efficiently #73"](https://github.com/facebookresearch/llama/pull/73/files)

## Different Languages

- [OpenLMLab/OpenChineseLLaMA](https://github.com/OpenLMLab/OpenChineseLLaMA) - This project is based on LLaMA-7B and the base of the Chinese large language model generated by the incremental pre-training of the Chinese dataset.
- [galatolofederico/vanilla-llama](https://github.com/galatolofederico/vanilla-llama) - `vanilla-llama` is a plain-pytorch implementation of `LLaMA` with minimal differences with respect to the original Facebook's implementation.
- [IPFS, PyLLaMA, Torrenting Thread "Where can I download the weights of the 7B model? #149"](https://github.com/facebookresearch/llama/issues/149)
- [HuggingFace/LLaMa](https://huggingface.co/docs/transformers/main/en/model_doc/llama)

## Research

### Most notable ideas,

1. Introduction,
  1. "... the best performances are not achieved by the largest models, but by smaller models trained on more data."
  2. "... the preferred model is not the fastest to train but the fastest at inference, and although it may be cheaper to train a large model to reach a certain level of performance, a smaller one trained longer will ultimately be cheaper at inference"
  3. "For instance, although Hoffmann et al. (2022) recommends training a 10B model on 200B tokens, we find that the performance of a 7B model continues to improve even after 1T tokens."
  4. "LLaMA, ranges from 7B to 65B parameters with competitive performance compared to the best existing LLMs"
  5. "we only use publicly available data, making our work compatible with open-sourcing"
  6. "we present an overview of the modifications we made to the transformer architecture"

### Papers

- [Hoffman et al. (2022)](https://arxiv.org/abs/2203.15556) - "Training Compute-Optimal Large Language Models"
> We determine the ideal transformer language model size and token count for a given compute budget. The recent emphasis on scaling language models while keeping training data constant has resulted in undertrained massive language models. We found that for compute-optimal training, model size and training tokens should be scaled equally: for every doubling of model size, the number of training tokens should also double.
