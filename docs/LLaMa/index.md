# LLaMA

- [Introducing LLaMA: A foundational, 65-billion-parameter large language model](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)

## Reproductions

1. GPTQ-for-LLaMa
2. llama.cpp
3. LLaMA-Adapter V2
4. Lit-LLaMa
5. StackLLaMa
6. The Bloke, `alpaca-lora-65B-GGML`

## Links

- [Read The Paper](https://arxiv.org/abs/2302.13971)
- [Read The Model Card](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md)
- [Apply For Access To LLaMa](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform)

## Relevant Reading

- [A LLaMa That Spits Out Posts: OUR TEST OF META'S AI](https://les-enovateurs.com/llama-meta-ai-test)
- [A Stranger Shares A Torrent Link For The Models, Transfomer, "Save bandwidth by using a torrent to distribute more efficiently #73"](https://github.com/facebookresearch/llama/pull/73/files)

## Different Languages

- [OpenLMLab/OpenChineseLLaMA](https://github.com/OpenLMLab/OpenChineseLLaMA) - This project is based on LLaMA-7B and the base of the Chinese large language model generated by the incremental pre-training of the Chinese dataset.
- [galatolofederico/vanilla-llama](https://github.com/galatolofederico/vanilla-llama) - `vanilla-llama` is a plain-pytorch implementation of `LLaMA` with minimal differences with respect to the original Facebook's implementation.
- [IPFS, PyLLaMA, Torrenting Thread "Where can I download the weights of the 7B model? #149"](https://github.com/facebookresearch/llama/issues/149)
- [HuggingFace/LLaMa](https://huggingface.co/docs/transformers/main/en/model_doc/llama)

## Research

### Papers

- [Hoffman et al. (2022)](https://arxiv.org/abs/2203.15556) - "Training Compute-Optimal Large Language Models"
> We determine the ideal transformer language model size and token count for a given compute budget. The recent emphasis on scaling language models while keeping training data constant has resulted in undertrained massive language models. We found that for compute-optimal training, model size and training tokens should be scaled equally: for every doubling of model size, the number of training tokens should also double.
