# LLaMA

- [Introducing LLaMA: A foundational, 65-billion-parameter large language model](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)

## Reproductions

1. GPTQ-for-LLaMa
2. llama.cpp
3. LLaMA-Adapter V2
4. Lit-LLaMa
5. StackLLaMa
6. The Bloke, `alpaca-lora-65B-GGML`
7. Baize
  1. [Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data](https://arxiv.org/abs/2304.01196)
  2. [project-baize/baize-chatbot](https://github.com/project-baize/baize-chatbot)
  3. [KdNuggets - Baize: An Open-Source Chat Model (But Different?)](https://www.kdnuggets.com/2023/04/baize-opensource-chat-model-different.html)

## Links

- [Read The Paper](https://arxiv.org/abs/2302.13971)
- [Read The Model Card](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md)
- [Apply For Access To LLaMa](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform)

## Relevant Reading

- [A LLaMa That Spits Out Posts: OUR TEST OF META'S AI](https://les-enovateurs.com/llama-meta-ai-test)
- [A Stranger Shares A Torrent Link For The Models, Transfomer, "Save bandwidth by using a torrent to distribute more efficiently #73"](https://github.com/facebookresearch/llama/pull/73/files)

## Different Languages

- [OpenLMLab/OpenChineseLLaMA](https://github.com/OpenLMLab/OpenChineseLLaMA) - This project is based on LLaMA-7B and the base of the Chinese large language model generated by the incremental pre-training of the Chinese dataset.
- [galatolofederico/vanilla-llama](https://github.com/galatolofederico/vanilla-llama) - `vanilla-llama` is a plain-pytorch implementation of `LLaMA` with minimal differences with respect to the original Facebook's implementation.
- [IPFS, PyLLaMA, Torrenting Thread "Where can I download the weights of the 7B model? #149"](https://github.com/facebookresearch/llama/issues/149)
- [HuggingFace/LLaMa](https://huggingface.co/docs/transformers/main/en/model_doc/llama)

## Research

### Most notable ideas,

1. Introduction,
  1. "... the best performances are not achieved by the largest models, but by smaller models trained on more data."
  2. "... the preferred model is not the fastest to train but the fastest at inference, and although it may be cheaper to train a large model to reach a certain level of performance, a smaller one trained longer will ultimately be cheaper at inference"
  3. "For instance, although Hoffmann et al. (2022) recommends training a 10B model on 200B tokens, we find that the performance of a 7B model continues to improve even after 1T tokens."
  4. "LLaMA, ranges from 7B to 65B parameters with competitive performance compared to the best existing LLMs"
  5. "we only use publicly available data, making our work compatible with open-sourcing"
  6. "we present an overview of the modifications we made to the transformer architecture"

### Papers

- [Hoffman et al. (2022)](https://arxiv.org/abs/2203.15556) - "Training Compute-Optimal Large Language Models"
> We determine the ideal transformer language model size and token count for a given compute budget. The recent emphasis on scaling language models while keeping training data constant has resulted in undertrained massive language models. We found that for compute-optimal training, model size and training tokens should be scaled equally: for every doubling of model size, the number of training tokens should also double.
- [Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762) - "Attention Is All You Need"
> Complex encoder-decoder recurrent or convolutional neural networks are the leading sequence transduction models. Attention links the encoder and decoder in the best models. The Transformer, a basic network design based on attention processes, eliminates repetition and convolutions. These models perform better on two machine translation tasks, are more parallelizable, and train faster. Our model outperforms the best results, including ensembles, on the WMT 2014 English-to-German translation task with 28.4 BLEU. After training for 3.5 days on eight GPUs, our model achieves a new state-of-the-art BLEU score of 41.8 on the WMT 2014 English-to-French translation challenge.
